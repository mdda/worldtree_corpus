{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ajm4Z6YUt_K"
   },
   "source": [
    "##TextGraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MjPluxS_spyh"
   },
   "source": [
    "###Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "s5W0IvX_k62R",
    "outputId": "7a23120b-565b-4698-efed-5adfae2c0e97"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "from pathlib import Path\n",
    "import spacy \n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import re\n",
    "\n",
    "path_data = \"tg2019task/worldtree_corpus_textgraphs2019sharedtask_withgraphvis\"\n",
    "if not Path(path_data).exists():\n",
    "    # Download data\n",
    "    !git clone -q https://github.com/umanlp/tg2019task.git\n",
    "    !cd tg2019task/ && make dataset\n",
    "    # Run baseline tfidf (expected MAP: 0.054)\n",
    "    # The baseline only submits the top n=10 sentences per question by default\n",
    "    # Increasing n to 5000 results in MAP >= 0.24 (cut short due to slow processing)\n",
    "    !cd {path_data} && python ../baseline_tfidf.py annotation/expl-tablestore-export-2017-08-25-230344/tables questions/ARC-Elementary+EXPL-Dev.tsv > predict.txt\n",
    "    !cd {path_data} && python ../evaluate.py --gold=questions/ARC-Elementary+EXPL-Dev.tsv predict.txt\n",
    "\n",
    "sys.path.append(\"tg2019task\")\n",
    "from baseline_tfidf import read_explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "56G1A3Ryk62W",
    "outputId": "2f985b7e-dec1-43d2-d130-244aeee5c872"
   },
   "outputs": [],
   "source": [
    "# Embedding and Spacy tokenizer\n",
    "def embed_texts(texts):\n",
    "    # Wrap with Keras model for convenient batching and progress bar\n",
    "    # Adapted from \"Keras + Universal Sentence Encoder = Transfer Learning for text data\"\n",
    "    texts = np.asarray(texts)\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"\n",
    "    embed = hub.Module(module_url)\n",
    "    \n",
    "    def UniversalEmbedding(x):\n",
    "        return embed(tf.squeeze(tf.cast(x, tf.string)), \n",
    "            signature=\"default\", as_dict=True)[\"default\"]\n",
    "    \n",
    "    inp = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "    out = tf.keras.layers.Lambda(UniversalEmbedding)(inp)\n",
    "    model = tf.keras.Model(inp, out)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        tf.keras.backend.set_session(sess)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "    \n",
    "        embeds = model.predict(texts, batch_size=128, verbose=True)\n",
    "        \n",
    "    tf.keras.backend.clear_session()  # avoid session closed error\n",
    "    return embeds\n",
    "\n",
    "def test_embed_texts():\n",
    "    print(embed_texts([\"hello\", \"bye\"]))\n",
    "    \n",
    "test_embed_texts()\n",
    "    \n",
    "def preprocess_texts(texts, remove_stop=True, remove_punct=True):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    tokens = []\n",
    "    lemmas = []\n",
    "    for doc in nlp.pipe(texts, disable=[\"ner\", \"tagger\", \"parser\"]):\n",
    "        _tokens = []\n",
    "        _lemmas = []\n",
    "        for token in doc:\n",
    "            if token.is_stop and remove_stop:\n",
    "                continue\n",
    "            if token.is_punct and remove_punct:\n",
    "                continue\n",
    "            _tokens.append(token.text)\n",
    "            _lemmas.append(token.lemma_)\n",
    "        tokens.append(_tokens)\n",
    "        lemmas.append(_lemmas)\n",
    "\n",
    "    return tokens, lemmas\n",
    "\n",
    "def test_preprocess_texts():\n",
    "    print(preprocess_texts([\"Which of these will most likely increase?\", \"Habitats support animals.\"]))\n",
    "    \n",
    "test_preprocess_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "colab_type": "code",
    "id": "GM1G1c_jk62b",
    "outputId": "42542705-043d-41d4-c861-210490b5ec98"
   },
   "outputs": [],
   "source": [
    "# Extract fact sentences\n",
    "path_tables = Path(path_data).joinpath(\"annotation/expl-tablestore-export-2017-08-25-230344/tables\")\n",
    "\n",
    "def get_df_explanations(path_tables):\n",
    "    explanations = []\n",
    "    for p in path_tables.iterdir():\n",
    "        explanations += read_explanations(str(p))\n",
    "    df = pd.DataFrame(explanations, columns=(\"uid\", \"text\"))  # 3 duplicate uids\n",
    "    df = df.drop_duplicates(\"uid\")\n",
    "    tokens, lemmas = preprocess_texts(df.text)\n",
    "    embeds = [row for row in embed_texts(df.text.tolist())]\n",
    "    df[\"tokens\"], df[\"lemmas\"], df[\"embedding\"] = tokens, lemmas, embeds\n",
    "    print(df.shape)\n",
    "    return df\n",
    "\n",
    "df_exp = get_df_explanations(path_tables)\n",
    "uid2idx = {uid: idx for idx, uid in enumerate(df_exp.uid.values)}\n",
    "print(list(uid2idx.items())[:3])\n",
    "df_exp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "id": "DDldq7nok62f",
    "outputId": "3b62cdbd-9194-4eef-f03d-6de941d4ea64"
   },
   "outputs": [],
   "source": [
    "# Format question examples\n",
    "path_questions = Path(path_data).joinpath(\"questions\")\n",
    "\n",
    "def extract_explanation(exp_string):\n",
    "    if type(exp_string) != str:\n",
    "        return [], []\n",
    "    uids = []\n",
    "    roles = []\n",
    "    for uid_and_role in exp_string.split():\n",
    "        uid, role = uid_and_role.split(\"|\")\n",
    "        uids.append(uid)\n",
    "        roles.append(role)\n",
    "    return uids, roles\n",
    "\n",
    "def split_question(q_string):\n",
    "    # split on option parantheses eg \"(A)\"\n",
    "    return re.compile(\"\\(.\\)\").split(q_string)\n",
    "\n",
    "def test_split_question():\n",
    "    print(split_question('Which process? (A) flying (B) talking (C) seeing (D) reproducing (E) something'))\n",
    "    print(split_question('Which process? (A) flying (B) talking (C) seeing (D) reproducing'))\n",
    "    print(split_question('Which process? (A) flying (B) talking (C) seeing'))\n",
    "    \n",
    "test_split_question()\n",
    "\n",
    "def get_questions(path_questions, fname):\n",
    "    df = pd.read_csv(Path(path_questions).joinpath(fname), sep=\"\\t\")\n",
    "    \n",
    "    # Reformat question\n",
    "    q_reformat = []\n",
    "    questions = df.Question.values\n",
    "    answers = df[\"AnswerKey.1\"].values\n",
    "    char2idx = {char: idx for idx, char in enumerate(list(\"ABCDE\"))}\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        q, *options = split_question(questions[i])\n",
    "        idx_option = char2idx[answers[i]]\n",
    "        q_reformat.append(\" \".join([q.strip(), options[idx_option].strip()]))\n",
    "    df[\"q_reformat\"] = q_reformat\n",
    "\n",
    "    # Preprocess texts\n",
    "    tokens, lemmas = preprocess_texts(df.q_reformat)\n",
    "    embeds = [row for row in embed_texts(df.q_reformat.tolist())]\n",
    "    df[\"tokens\"], df[\"lemmas\"], df[\"embedding\"] = tokens, lemmas, embeds\n",
    "\n",
    "    # Get explanation uids and roles\n",
    "    exp_uids = []\n",
    "    exp_roles = []\n",
    "    exp_idxs = []\n",
    "    for exp_string in df.explanation.values:\n",
    "        _uids, _roles = extract_explanation(exp_string)\n",
    "        uids = []\n",
    "        roles = []\n",
    "        idxs = []\n",
    "        assert len(_uids) == len(_roles)\n",
    "        for i in range(len(_uids)):\n",
    "            if _uids[i] not in uid2idx:\n",
    "                continue\n",
    "            uids.append(_uids[i])\n",
    "            roles.append(_roles[i])\n",
    "            idxs.append(uid2idx[_uids[i]])\n",
    "        exp_uids.append(uids)\n",
    "        exp_roles.append(roles)\n",
    "        exp_idxs.append(idxs)\n",
    "    df[\"exp_uids\"], df[\"exp_roles\"], df[\"exp_idxs\"] = exp_uids, exp_roles, exp_idxs\n",
    "\n",
    "    print(df.shape)\n",
    "    return df\n",
    "\n",
    "df_trn = get_questions(path_questions, \"ARC-Elementary+EXPL-Train.tsv\")\n",
    "df_dev = get_questions(path_questions, \"ARC-Elementary+EXPL-Dev.tsv\")\n",
    "df_test = get_questions(path_questions, \"ARC-Elementary+EXPL-Test-Masked.tsv\")\n",
    "df_trn.sample(3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "colab_type": "code",
    "id": "NZiNocTHk62j",
    "outputId": "395dcc38-613f-485b-80ee-d2da9771a9bc"
   },
   "outputs": [],
   "source": [
    "# Build unique lemma/token data\n",
    "def flatten(nested_list):\n",
    "    return [item for lst in nested_list for item in lst]\n",
    "\n",
    "def get_flattened_items(dfs, field):\n",
    "    all_items = []\n",
    "    for df in dfs:\n",
    "        all_items.extend(flatten(df[field]))\n",
    "    print(len(all_items))\n",
    "    return all_items\n",
    "\n",
    "all_lemmas = get_flattened_items([df_trn, df_dev, df_test, df_exp], \"lemmas\")\n",
    "# all_tokens = get_flattened_items([df_trn, df_dev, df_test, df_exp], \"tokens\")\n",
    "    \n",
    "unique_lemmas = sorted(list(set(all_lemmas)))\n",
    "print(len(unique_lemmas))\n",
    "lemma2id = {lemma:idx for idx, lemma in enumerate(unique_lemmas)}\n",
    "df_lemma = pd.DataFrame({\n",
    "    \"node\": unique_lemmas,\n",
    "    \"embedding\": [row for row in embed_texts(unique_lemmas)]\n",
    "})\n",
    "\n",
    "# Extract lemma node ids for every sentence\n",
    "def get_nodes(lemmas):\n",
    "    return [lemma2id[lemma] for lemma in lemmas]\n",
    "\n",
    "def add_nodes(df):\n",
    "    df[\"nodes\"] = df.lemmas.apply(get_nodes)\n",
    "\n",
    "add_nodes(df_trn)\n",
    "add_nodes(df_dev)\n",
    "add_nodes(df_test)\n",
    "add_nodes(df_exp)\n",
    "\n",
    "def test_nodes(i=0):\n",
    "    print(df_trn.q_reformat.iloc[i])\n",
    "    print(df_lemma.iloc[df_trn.nodes.iloc[i]])\n",
    "    \n",
    "test_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "AyCNurQL4-E2",
    "outputId": "250c49d5-f04a-406f-89c3-4890d18418c6"
   },
   "outputs": [],
   "source": [
    "# Nearest neighbours from embedding\n",
    "!pip install -q annoy\n",
    "from annoy import AnnoyIndex\n",
    "annoy_index = AnnoyIndex(512, \"angular\")\n",
    "for i in range(len(df_exp)):\n",
    "    annoy_index.add_item(i, df_exp.embedding.iloc[i])\n",
    "annoy_index.build(10)\n",
    "\n",
    "# Add nearest neighbour explanation ids for each embedding\n",
    "def add_nn(df, n=100):\n",
    "    df[\"nn_exp\"] = df.embedding.apply(\n",
    "        lambda emb: annoy_index.get_nns_by_vector(emb, n))\n",
    "    \n",
    "add_nn(df_trn)\n",
    "add_nn(df_dev)\n",
    "add_nn(df_test)\n",
    "add_nn(df_exp)\n",
    "\n",
    "def test_annoy(i=0):\n",
    "    print(df_trn.q_reformat.iloc[i])\n",
    "    print(df_exp.iloc[df_trn.nn_exp.iloc[i]].text.values[:3])\n",
    "    \n",
    "test_annoy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "id": "BLO8ttCqjTzV",
    "outputId": "fd51106e-0c2a-42f2-ce08-a54856f42ac8"
   },
   "outputs": [],
   "source": [
    "# Example question\n",
    "q = df_trn.sample(1, random_state=42)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "colab_type": "code",
    "id": "AxxLtXRJjlmN",
    "outputId": "bbde7911-2307-4a3e-9c8e-c60858fbf5ce"
   },
   "outputs": [],
   "source": [
    "# Get facts for question\n",
    "e = df_exp.iloc[q.exp_idxs.iloc[0]]\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "id": "_gccHRjxkOyF",
    "outputId": "14b60c3e-0819-4980-d9d2-323a4fa5ca19"
   },
   "outputs": [],
   "source": [
    "# Get lemma token nodes for first sentence\n",
    "n = df_lemma.iloc[e.nodes.iloc[0]]\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2sDIf5QCk62F"
   },
   "source": [
    "###Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rl7J4ER9Wptf"
   },
   "source": [
    "####Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_mGmuHCwWgK"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Start with question\n",
    "Get seed, n most relevant\n",
    "Add to ranking dict with idx\n",
    "For each item in ranking dict, add the nearest neighbour to ranking dict that is not already inside\n",
    "Repeat until no more change\n",
    "\"\"\"\n",
    "\n",
    "def repeat(seed):\n",
    "    ranking = {item: idx for idx, item in enumerate(seed)}\n",
    "    nns = {item: list(df_exp.nn_exp.iloc[item]) for item in range(len(df_exp))}\n",
    "    \n",
    "    while True:\n",
    "        old_ranking = dict(ranking)\n",
    "#         print(len(old_ranking), old_ranking)\n",
    "        \n",
    "        for item in old_ranking.keys():\n",
    "            while True:\n",
    "                if len(nns[item]) == 0:\n",
    "                    break\n",
    "                n = nns[item].pop(0)\n",
    "                if n not in ranking:\n",
    "                    ranking[n] = len(ranking)\n",
    "                    break\n",
    "        if len(ranking) == len(old_ranking):\n",
    "            break\n",
    "            \n",
    "    # low rank -> high importance -> high score\n",
    "    scores = {k: -v for k, v in ranking.items()}\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qFc48n40RJfX",
    "outputId": "89276998-d98a-4bb9-e926-4c9d428d584d"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Start with question\n",
    "Get n most relevant/similar\n",
    "+1 point each\n",
    "Recurse up to iter times \n",
    "Final rank on points\n",
    "(MAP=0.04)\n",
    "\"\"\"\n",
    "\n",
    "def recurse(seed, scores, iter, n=100, max_iter=2):\n",
    "    if iter < max_iter:    \n",
    "        for idx in seed[:n]:\n",
    "            if idx in scores:\n",
    "                scores[idx] += 1\n",
    "            else:\n",
    "                scores[idx] = 1\n",
    "\n",
    "        for idx in seed[:n]:\n",
    "            if scores[idx] > 1:\n",
    "                continue\n",
    "            new_seed = df_exp.nn_exp.iloc[idx][1:]  # skip nearest (itself)\n",
    "            recurse(new_seed, scores, iter+1, n, max_iter)\n",
    "        \n",
    "def get_ranking(scores_dict):\n",
    "    idxs = list(scores_dict.keys())\n",
    "    idxs = sorted(idxs, key=lambda idx: scores_dict[idx], reverse=True)\n",
    "    return {idx: rank for rank, idx in enumerate(idxs)}\n",
    "        \n",
    "def test_recurse(i=0):\n",
    "    # Time: about 1s to run 100x\n",
    "    print(\"\\nQuestion:\", df_trn.q_reformat.iloc[i])\n",
    "    nearest = df_trn.nn_exp.iloc[i]\n",
    "    \n",
    "    scores = {}\n",
    "    recurse(seed=nearest, scores=scores, iter=0)\n",
    "    \n",
    "#     scores = repeat(nearest)  # test the output of repeat algo\n",
    "\n",
    "    print(\"\\nNum sentences seen:\", len(scores), \"\\nScores:\", scores)\n",
    "    ranking = get_ranking(scores)\n",
    "    \n",
    "#     print(\"\\nSample of initial seeds:\")\n",
    "#     for idx in nearest[:5]:\n",
    "#         print(\"Pred rank:\", ranking.get(idx), \"\\tText:\", df_exp.iloc[idx].text)\n",
    "    \n",
    "    n_show = 10\n",
    "    print(\"\\nTop ranking:\", n_show)\n",
    "    idxs = list(scores.keys())\n",
    "    print(df_exp.iloc[sorted(idxs, key=lambda idx: scores[idx], reverse=True)[:n_show]].text.values)\n",
    "    \n",
    "    print(\"\\nGold explanations\")\n",
    "    for idx in df_trn.iloc[i].exp_idxs:\n",
    "        print(\"Pred rank:\", ranking.get(idx), \"\\tText:\", df_exp.iloc[idx].text)\n",
    "    \n",
    "for i in np.random.choice(len(df_trn), 10):\n",
    "    test_recurse(i)\n",
    "    print(\"#\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QkmTlJ4-6gxo",
    "outputId": "0c7e0dc1-ab51-4de6-b481-bff960fb80b9"
   },
   "outputs": [],
   "source": [
    "def simple_nn_ranking(df, idx):\n",
    "    # 1000x takes 3.4s\n",
    "    return annoy_index.get_nns_by_vector(df.embedding.iloc[idx], n=len(df_exp))\n",
    "\n",
    "def test_simple_nn_ranking(i=0):\n",
    "    # Time: about 1s to run 100x\n",
    "    print(\"\\nQuestion:\", df_trn.q_reformat.iloc[i])\n",
    "    ranking = simple_nn_ranking(df_trn, i)\n",
    "    \n",
    "    n_show = 10\n",
    "    print(\"\\nTop ranking:\", n_show)\n",
    "    print(df_exp.iloc[ranking[:n_show]].text.values)\n",
    "    \n",
    "    print(\"\\nGold explanations\")\n",
    "    for idx in df_trn.iloc[i].exp_idxs:\n",
    "        print(\"Pred rank:\", ranking.index(idx), \"\\tText:\", df_exp.iloc[idx].text)\n",
    "    \n",
    "for i in np.random.choice(len(df_trn), 10):\n",
    "    test_simple_nn_ranking(i)\n",
    "    print(\"#\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LzPwKEOjAGKl"
   },
   "outputs": [],
   "source": [
    "# # Predicting combined embedding of answer sentences then nearest neighbour ranking (MAP=0.17)\n",
    "# def make_xy(df):\n",
    "#     embs_q = []\n",
    "#     concat_sentences = []\n",
    "    \n",
    "#     def concat(sentences):\n",
    "#         # assuming sentences have no punctuation at the end\n",
    "#         return \". \".join([s.strip() for s in sentences])\n",
    "    \n",
    "#     for i in range(len(df)):\n",
    "#         idxs_exp = df.exp_idxs.iloc[i]\n",
    "#         sentences = df_exp.text.iloc[idxs_exp].values\n",
    "#         if len(sentences) == 0:\n",
    "#             continue\n",
    "#         concat_sentences.append(concat(sentences))\n",
    "#         embs_q.append(df.embedding.iloc[i])\n",
    "        \n",
    "#     embs_concat_sentences = embed_texts(concat_sentences)\n",
    "    \n",
    "#     x = np.stack(embs_q)\n",
    "#     y = embs_concat_sentences\n",
    "#     print(x.shape, y.shape)\n",
    "#     return x, y\n",
    "    \n",
    "# # def test_make_xy():\n",
    "# #     make_xy(df_trn[:10])\n",
    "    \n",
    "# # test_make_xy()\n",
    "\n",
    "# x_trn, y_trn = make_xy(df_trn)\n",
    "# x_dev, y_dev = make_xy(df_dev)\n",
    "\n",
    "# from sklearn import linear_model, metrics\n",
    "\n",
    "# model = linear_model.Ridge()\n",
    "# model.fit(x_trn, y_trn)\n",
    "\n",
    "# import scipy\n",
    "\n",
    "# def cosine_metric(y_true, y_pred):\n",
    "#     assert len(y_true) == len(y_pred)\n",
    "    \n",
    "#     pairwise_matrix = metrics.pairwise.cosine_similarity(y_true, y_pred)\n",
    "#     return np.mean([pairwise_matrix[i][i] for i in range(len(y_true))])\n",
    "\n",
    "#     # Equivalent\n",
    "#     # return 1 - np.mean([scipy.spatial.distance.cosine(a, b) for a, b in zip(y_true, y_pred)])\n",
    "\n",
    "# print(\"Cosine similarity from question to embeds of concat sentences:\", cosine_metric(x_dev, y_dev))\n",
    "# print(\"Cosine similarity from pred emb to embeds of concat sentences:\", cosine_metric(y_dev, model.predict(x_dev)))\n",
    "\n",
    "# def pred_emb_nn_ranking(df, idx, n=5000):\n",
    "#     inp = [df.embedding.iloc[idx]]\n",
    "#     out = model.predict(inp)\n",
    "#     pred_emb = out[0]\n",
    "#     return annoy_index.get_nns_by_vector(pred_emb, n=len(df_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0QLofodZf2FO",
    "outputId": "ba56d0e1-456d-4d19-8466-72174ff09783"
   },
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction, metrics\n",
    "\n",
    "def get_tfidf_ranking(df, field_q=\"q_reformat\", field_e=\"text\"):\n",
    "    # \"q_reformat\" instead of \"Question\"      MAP +0.07 (0.24 -> 0.31)\n",
    "    # tfidf stop_words=\"english\"              MAP +0.01 (0.31 -> 0.32)\n",
    "    # field_q and field_e = \"lemmas\"          MAP +0.08 (0.32 -> 0.40)\n",
    "    # tfidf binary=True                       MAP +0.03 (0.40 -> 0.43)\n",
    "    # tfidf ngram_range=(1,2)                 MAP -0.05 (0.43 -> 0.38)\n",
    "    # tfidf ngram_range=(1,3)                 MAP -0.07 (0.43 -> 0.36)\n",
    "    \n",
    "    def preprocess(lst):\n",
    "        if type(lst[0]) == str:\n",
    "            return lst\n",
    "        elif type(lst[0]) == list:\n",
    "            return [\" \".join(sublst) for sublst in lst]\n",
    "        else:\n",
    "            raise TypeError(\"unknown data type\")\n",
    "    \n",
    "    ranking = []\n",
    "    q = preprocess(df[field_q].tolist())\n",
    "    e = preprocess(df_exp[field_e].tolist())\n",
    "    \n",
    "    vectorizer = feature_extraction.text.TfidfVectorizer(\n",
    "        stop_words=\"english\", binary=True)\n",
    "    vectorizer.fit(q + e)\n",
    "    X_q = vectorizer.transform(q)\n",
    "    X_e = vectorizer.transform(e)\n",
    "    X_dist = metrics.pairwise.cosine_distances(X_q, X_e)\n",
    "    \n",
    "    for i_question, distances in enumerate(X_dist):\n",
    "        ranking.append([])\n",
    "        for i_explanation in np.argsort(distances):\n",
    "            ranking[-1].append(i_explanation)\n",
    "\n",
    "    return ranking\n",
    "\n",
    "def add_missing_idxs(old):\n",
    "    set_old = set(old)\n",
    "    set_all = set(np.arange(0, len(df_exp)))\n",
    "    missing = list(set_all - set_old)\n",
    "    np.random.shuffle(missing)\n",
    "    new = list(old)\n",
    "    new.extend(missing)\n",
    "    assert len(new) == len(df_exp), (len(new), len(df_exp), len(missing))\n",
    "    assert all([a == b  for a, b in zip(new[:len(old)], old)])\n",
    "    assert set(new) == set_all\n",
    "    return new\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def format_predict_line(questionID, explanation_uid):\n",
    "    # Adapted from tfidf baseline script\n",
    "    return \"{}\\t{}\".format(questionID, explanation_uid)\n",
    "\n",
    "def get_preds(df, df_exp):\n",
    "    preds = []\n",
    "    tfidf_ranking = get_tfidf_ranking(df)\n",
    "    tfidf_ranking = get_tfidf_ranking(df, \"lemmas\", \"lemmas\")\n",
    "    \n",
    "    for i in tqdm(range(len(df))):\n",
    "        \n",
    "#         # Repeating nn algo (23.7)\n",
    "#         nearest = df.nn_exp.iloc[i]\n",
    "#         scores = repeat(nearest)\n",
    "#         ranked_idxs = sorted(list(scores.keys()), key=lambda idx: scores[idx], reverse=True)\n",
    "        \n",
    "#         # Recursive nn algo (0.07)\n",
    "#         nearest = df.nn_exp.iloc[i]\n",
    "#         scores = {}\n",
    "#         recurse(seed=nearest, scores=scores, iter=0)\n",
    "#         ranked_idxs = sorted(list(scores.keys()), key=lambda idx: scores[idx], reverse=True)\n",
    "        \n",
    "#         # Simple nn algo (0.24)\n",
    "#         ranked_idxs = simple_nn_ranking(df, i)\n",
    "        \n",
    "#         # Predict concat answer embedding algo (0.17)\n",
    "#         ranked_idxs = pred_emb_nn_ranking(df, i)\n",
    "\n",
    "        # Tfidf\n",
    "        ranked_idxs = tfidf_ranking[i]\n",
    "\n",
    "        if len(ranked_idxs) < len(df_exp):\n",
    "            ranked_idxs = add_missing_idxs(ranked_idxs)\n",
    "        \n",
    "        questionID = df.questionID.iloc[i]\n",
    "        for idx in ranked_idxs:\n",
    "            preds.append(format_predict_line(questionID, df_exp.uid.iloc[idx]))\n",
    "    return preds\n",
    "        \n",
    "def test_get_preds():\n",
    "    preds = get_preds(df_dev[:10], df_exp)\n",
    "    print(\"Num preds:\", len(preds))\n",
    "    print(\"Prediction lines:\", preds)\n",
    "    \n",
    "test_get_preds()\n",
    "\n",
    "def write_preds(preds, path=\"predict.txt\"):\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(preds))\n",
    "        \n",
    "def test_write_preds():\n",
    "    preds = ['VASoL_2008_3_26\\t14de-6699-6b2e-a5d1', 'VASoL_2008_3_26\\t14de-6699-6b2e-a5d1']\n",
    "    path = \"temp.txt\"\n",
    "    write_preds(preds, path)\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            print(repr(line))\n",
    "            \n",
    "test_write_preds()\n",
    "\n",
    "write_preds(get_preds(df_trn, df_exp))  # for dev set\n",
    "# write_preds(get_preds(df_dev, df_exp))  # for dev set\n",
    "# write_preds(get_preds(df_dev, df_exp))  # for test set\n",
    "!cd {path_data} && python ../evaluate.py --gold=questions/ARC-Elementary+EXPL-Train.tsv /content/predict.txt\n",
    "# !cd {path_data} && python ../evaluate.py --gold=questions/ARC-Elementary+EXPL-Dev.tsv /content/predict.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C5xJxh9Dv19X"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3ajm4Z6YUt_K",
    "MjPluxS_spyh",
    "2sDIf5QCk62F",
    "peKpyaxKvwVu",
    "1GxJNXVWREUK",
    "rl7J4ER9Wptf",
    "N44taAQdTkWr",
    "z17f8Fwmvt5h",
    "saX02KWMwICp",
    "aUYa501gwRBy",
    "S1N3TvdOxChq",
    "g5e7EqMRHWNd",
    "hX4VX793h3qF"
   ],
   "name": "textgraphs_submission_090819.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
