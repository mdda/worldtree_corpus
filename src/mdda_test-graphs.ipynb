{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regenerate=False\n",
    "statements = dataset.load_statements(regenerate=regenerate)\n",
    "statements_by_uid = { s.uid:s for s in statements }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_counts = dataset.get_keyword_counts_from_statements(statements)\n",
    "len(keyword_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i, s in enumerate(statements):\n",
    "#    print(s)\n",
    "#    if i>10: break\n",
    "G.add_nodes_from(s.uid for s in statements)\n",
    "len(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_nodes_from(k for k,c in keyword_counts.items())\n",
    "len(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://networkx.github.io/documentation/stable/reference/classes/generated/\n",
    "#   networkx.Graph.add_weighted_edges_from.html\n",
    "for i, s in enumerate(statements):\n",
    "    G.add_weighted_edges_from(\n",
    "        #(k, s.uid, 1.0) for k in s.keywords  # Evenly weighted :: Poor\n",
    "        #(k, s.uid, 1.0+i) for k in s.keywords  # Demonstrates that weightings are 'weight'\n",
    "        #(k, s.uid, 2.0-1.0/keyword_counts[k]) for k in s.keywords  # weight by how frequent keywords are\n",
    "        #(k, s.uid, 3.0-1.0/len(s.keywords)-1.0/keyword_counts[k]) for k in s.keywords   # bit of both\n",
    "        # Decent:\n",
    "        (k, s.uid, 2.0-1.0/len(s.keywords)) for k in s.keywords   # weight by how many keywords the statement has\n",
    "    )\n",
    "len(G.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effectively kill some nodes\n",
    "for node in ['be']:\n",
    "    G.add_weighted_edges_from((node, n, 100.) for n in G.neighbors(node) )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[n for n in G.neighbors('iron')]\n",
    "[n for n in G.neighbors('2ead-9402-4803-38bc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.dijkstra_path(G, 'iron', 'water')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in nx.all_shortest_paths(G, 'iron', 'water', weight='weight'): \n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_path(p):\n",
    "    for i,n in enumerate(p):\n",
    "        #if i%2>1:\n",
    "        if len(n)==19:  # a uid\n",
    "            print(\"   > \"+n+\" : \"+statements_by_uid[n].raw_txt)\n",
    "        else:\n",
    "            print(n)\n",
    "    print()\n",
    "\n",
    "explain_path( nx.dijkstra_path(G, 'rust', 'water') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in nx.all_shortest_paths(G, 'saltwater', 'reason', weight='weight'): \n",
    "    explain_path( p )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regenerate=False\n",
    "#qanda_train = dataset.load_qanda('train', regenerate=regenerate) # 1.8MB\n",
    "qanda_dev   = dataset.load_qanda('dev', regenerate=regenerate)   # 400k in 496 lines\n",
    "#qanda_test  = dataset.load_qanda('test', regenerate=regenerate)  # 800k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_baseline=dict() # qa_id -> [statements in order]\n",
    "with open('/tmp/scorer/predict.txt', 'rt') as f:\n",
    "    for l in f.readlines():\n",
    "        qid, uid = l.strip().split('\\t')\n",
    "        if qid not in preds_baseline: preds_baseline[qid]=[]\n",
    "        preds_baseline[qid].append(uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = qanda_dev[485]  #212  #4=astronomy\n",
    "\n",
    "print(qa.question_id, qa.flags)\n",
    "print(\"Q: \"+qa.question.raw_txt)\n",
    "print(\"  \", qa.question.keywords)\n",
    "for i,ans in enumerate(qa.answers):\n",
    "    print(f\"A{i:1d}: \"+ans.raw_txt)\n",
    "    print(\"  \", ans.keywords)\n",
    "print(\"explanation_gold:\")\n",
    "for ex in qa.explanation_gold:\n",
    "    print(f\"  {ex.uid} : {statements_by_uid[ex.uid].raw_txt}\")\n",
    "    print(f\"{' '*25} : {statements_by_uid[ex.uid].keywords}\")\n",
    "\n",
    "print()\n",
    "print(f\"\"\"baseline MAP = {dataset.silent_average_precision_score(\n",
    "    set(e.uid for e in qa.explanation_gold), preds_baseline[qa.question_id][:]):.4f}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements_by_uid[qa.explanation_gold[2].uid].keywords # .add('part')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "','.join(qa.question.keywords), ','.join(qa.answers[0].keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_all = qa.question.keywords | qa.answers[0].keywords\n",
    "kw_all = set([kw for kw in kw_all if kw in G.nodes]) # Only use those in the vocab ...\n",
    "','.join(kw_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target='core'\n",
    "#kw_all-set([target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d,p=nx.multi_source_dijkstra(G, kw_all-set([target]), target=target, weight='weight')\n",
    "d,p=nx.multi_source_dijkstra(G, ['component'], target='speedometer', weight='weight')\n",
    "#print(p)\n",
    "explain_path( p ) # This is not great..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kw_arr = list(kw_all)\n",
    "#srcs, tgts, limit_i_j = kw_all, kw_all, True\n",
    "srcs, tgts, limit_i_j = qa.question.keywords, qa.answers[0].keywords, False\n",
    "\n",
    "srcs = set([n for n in srcs if n in G.nodes]) # Only use those in the vocab ...\n",
    "tgts = set([n for n in tgts if n in G.nodes]) # Only use those in the vocab ...\n",
    "\n",
    "node_cnt=dict()\n",
    "for i, src in enumerate(srcs):\n",
    "    print(f\"{i}/{len(srcs)}\")\n",
    "    for j, tgt in enumerate(tgts):\n",
    "        if limit_i_j and i>=j: continue\n",
    "        for p in nx.all_shortest_paths(G, src, tgt, weight='weight'): \n",
    "            explain_path( p )\n",
    "            for k,n in enumerate(p):\n",
    "                if k%2==0:continue\n",
    "                if n not in node_cnt: node_cnt[n]=0\n",
    "                node_cnt[n]+=1\n",
    "len(node_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_desc = sorted(node_cnt.items(),key=lambda n:-n[1])\n",
    "#nodes_desc[:10]\n",
    "len(nodes_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ f\"{cnt:5d} : {uid:s} : {statements_by_uid[uid].raw_txt}\" for uid,cnt in nodes_desc ][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_ex_uid = set(e.uid for e in qa.explanation_gold)\n",
    "preds_uid   = preds_baseline[qa.question_id]\n",
    "\n",
    "def base_uids_list(arr):\n",
    "    base_arr=[]\n",
    "    for a in arr: \n",
    "        base=a[:19]\n",
    "        if base not in base_arr: base_arr.append(base)\n",
    "    return base_arr\n",
    "\n",
    "nodes_uid   = base_uids_list( [uid for uid,_ in nodes_desc] )\n",
    "\n",
    "for limit in [10000, 64, 32,16] :\n",
    "    sc_baseline= dataset.silent_average_precision_score(\n",
    "                   gold_ex_uid, preds_uid[:limit])\n",
    "    sc_graph   = dataset.silent_average_precision_score(\n",
    "                   gold_ex_uid, nodes_uid[:limit])\n",
    "    print(f\"{limit:5d} : baseline={sc_baseline:.4f}, graph={sc_graph:.4f}\")\n",
    "print(f\"recall:          {len(gold_ex_uid & set(preds_uid[:64]))/len(gold_ex_uid):.4f}\"+\n",
    "               f\"        {len(gold_ex_uid & set(nodes_uid[:64]))/len(gold_ex_uid):.4f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrees with baseline scoring... (good job too, since it's the actual scorer)\n",
    "# python ../tg2020task/evaluate.py --gold ../tg2020task/questions.dev.tsv /tmp/scorer/predict.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_distance_statements(qa, all_to_all=False):\n",
    "    if all_to_all:\n",
    "        kw_all = qa.question.keywords | qa.answers[0].keywords\n",
    "        #kw_all = set([kw for kw in kw_all if kw in G.nodes]) # Only use those in the vocab ...\n",
    "        #print(len(kw_all))\n",
    "        srcs, tgts, limit_i_j = kw_all, kw_all, True\n",
    "    else:\n",
    "        srcs, tgts, limit_i_j = qa.question.keywords, qa.answers[0].keywords, False\n",
    "\n",
    "    srcs = set([n for n in srcs if n in G.nodes]) # Only use those in the vocab ...\n",
    "    tgts = set([n for n in tgts if n in G.nodes]) # Only use those in the vocab ...\n",
    "        \n",
    "    node_cnt=dict()\n",
    "    for i, src in enumerate(srcs):\n",
    "        #print(f\"{i}/{len(srcs)}\")\n",
    "        for j, tgt in enumerate(tgts):\n",
    "            if limit_i_j and i>=j: continue\n",
    "            try:\n",
    "                paths = nx.all_shortest_paths(G, src, tgt, weight='weight')\n",
    "                for p in paths: \n",
    "                    #explain_path( p )\n",
    "                    for k,n in enumerate(p):\n",
    "                        #if k%2==0:continue\n",
    "                        if len(n)<19:continue  # Ignore the keywords\n",
    "                        if n not in node_cnt: node_cnt[n]=0\n",
    "                        node_cnt[n]+=1\n",
    "            except:\n",
    "                print(\"  Cannot get to \"+tgt)\n",
    "    nodes_desc = sorted(node_cnt.items(),key=lambda n:-n[1])\n",
    "    #print(nodes_desc)\n",
    "    nodes_uid   = base_uids_list( [uid for uid,_ in nodes_desc] )\n",
    "    return nodes_uid\n",
    "\n",
    "\n",
    "#qa = qanda_dev[4]  #212  #4=astronomy    \n",
    "if False:\n",
    "    #with open(\"/tmp/scorer/predict_graph.txt\", \"wt\") as f:\n",
    "    with open(\"../predictions/predict_graph-q-to-a.txt\", \"wt\") as f:\n",
    "        for qa_i, qa in enumerate(qanda_dev):\n",
    "            print(f\"Running : {qa_i}\")\n",
    "            uids = get_min_distance_statements(qa)  #, all_to_all=True) \n",
    "            for uid in uids:\n",
    "                f.write(f\"{qa.question_id}\\t{uid}\\n\")\n",
    "#get_min_distance_statements(qanda_dev[486])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python ../tg2020task/evaluate.py --gold ../tg2020task/questions.dev.tsv /tmp/scorer/predict_graph.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in dataset.nlp('if a substance has a higher density than another substance , then the molecules in the substance will be closer than those of the other substance'):\n",
    "#for t in dataset.nlp('About how many times does the moon orbit Earth in a year?'):\n",
    "#for t in dataset.nlp('approximately means about'):\n",
    "    print(t.pos_, t.text, t.lemma_)\n",
    "# ADP About about ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different idea : let's create a new graph from the QuestionAnswer and the known explanation_gold nodes\n",
    "#   Then : Have a look at nodes that only have 1 edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://networkx.github.io/documentation/stable/reference/classes/generated/networkx.Graph.subgraph.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_all = qa.question.keywords | qa.answers[0].keywords\n",
    "copy_qa = set([kw for kw in kw_all if kw in G.nodes]) # Only use those in the vocab ...\n",
    "len(copy_qa), ','.join(copy_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_gold = set([e.uid for e in qa.explanation_gold])\n",
    "len(copy_gold), ','.join(copy_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in copy_gold:\n",
    "    print(c, list(G.neighbors(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_adj = set([nbr for n in copy_gold for nbr in G.neighbors(n) ]) # Order of for loops v important\n",
    "len(copy_adj), ','.join(sorted(copy_adj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy_nodes = set([n for lst in [copy_gold, copy_adj, copy_qa] for n in lst])\n",
    "#len(copy_nodes), ','.join(sorted(copy_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SG = G.__class__()\n",
    "SG.add_nodes_from((n, G.nodes[n]) for n in copy_gold )   \n",
    "SG.add_nodes_from((n, G.nodes[n]) for n in copy_adj )   \n",
    "SG.add_nodes_from((n, G.nodes[n]) for n in copy_qa )   \n",
    "SG.add_edges_from((n, nbr, d)\n",
    "    for n, nbrs in G.adj.items() if n in copy_gold\n",
    "    for nbr, d in nbrs.items() if nbr in copy_adj)\n",
    "SG.add_edges_from((n, nbr, d)\n",
    "    for n, nbrs in G.adj.items() if n in copy_gold\n",
    "    for nbr, d in nbrs.items() if nbr in copy_qa)\n",
    "len(SG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for the nodes with only one edge:\n",
    "for n in SG.nodes:\n",
    "    l=len(list(SG.neighbors(n)))\n",
    "    if l<=1:\n",
    "        print(n, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lonely_nodes(ex_list, copy_qa=copy_qa, ):\n",
    "    copy_adj = set([nbr for n in ex_list for nbr in G.neighbors(n) ]) # Order of for loops v important\n",
    "    \n",
    "    SG = G.__class__()\n",
    "    SG.add_nodes_from((n, G.nodes[n]) for n in ex_list )   \n",
    "    SG.add_nodes_from((n, G.nodes[n]) for n in copy_adj )   \n",
    "    SG.add_nodes_from((n, G.nodes[n]) for n in copy_qa )   \n",
    "    SG.add_edges_from((n, nbr, d)\n",
    "        for n, nbrs in G.adj.items() if n in ex_list\n",
    "        for nbr, d in nbrs.items() if nbr in copy_adj)\n",
    "    SG.add_edges_from((n, nbr, d)\n",
    "        for n, nbrs in G.adj.items() if n in ex_list\n",
    "        for nbr, d in nbrs.items() if nbr in copy_qa)\n",
    "    #print(len(SG))\n",
    "    print(\"question keywords\", [n for n in (qa.question.keywords) if n in SG.nodes])\n",
    "    print(\"answers[0] keywords\", [n for n in (qa.answers[0].keywords) if n in SG.nodes])\n",
    "    print(\"leaf nodes\", [n for n in (SG.nodes - copy_qa) if len(list(SG.neighbors(n)))<=1 ])\n",
    "    \n",
    "    # qa.question.keywords | qa.answers[0].keywords\n",
    "    empty_q = [n for n in (qa.question.keywords)\n",
    "                if n in SG.nodes and len(list(SG.neighbors(n)))==0 ]\n",
    "    empty_a = [n for n in (qa.answers[0].keywords)\n",
    "                if n in SG.nodes and len(list(SG.neighbors(n)))==0 ]\n",
    "    lonely_leaf = set( [n for n in (SG.nodes - copy_qa)\n",
    "                    if len(list(SG.neighbors(n)))<=1 ] )\n",
    "    \n",
    "    # Links between explanation statements\n",
    "    # == non-qa and non-leaf words?\n",
    "    #  ?+? multi-linked qa words?\n",
    "    statement_links = [n for n in (SG.nodes - copy_qa - lonely_leaf)\n",
    "                                if len(n)!=19 ]\n",
    "    \n",
    "    return empty_q, empty_a, lonely_leaf, statement_links\n",
    "\n",
    "get_lonely_nodes(copy_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in sorted(copy_gold):\n",
    "    s = statements_by_uid[uid]\n",
    "    print(f\"  {uid} : {s.raw_txt}\")\n",
    "    print(f\"          {s.keywords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now look at the predictions from baseline_retrieval\n",
    "ex_guess = preds_baseline[qa.question_id][:5]\n",
    "get_lonely_nodes( ex_guess )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in ex_guess:\n",
    "    s = statements_by_uid[uid]\n",
    "    print(f\"  {uid} : {s.raw_txt}\")\n",
    "    print(f\"          {s.keywords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
