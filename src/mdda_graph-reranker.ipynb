{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2, suppress=True, floatmode='fixed', sign=' ')\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional, Dict, Tuple, Set, Union\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regenerate=False\n",
    "statements = dataset.load_statements(regenerate=regenerate)\n",
    "statements_by_uid = { s.uid:s for s in statements }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../tg2020task/tableindex.txt\", \"rt\") as f:\n",
    "    table_names:List[str] = ['Q', 'A-right', 'A-wrong', ]  \n",
    "    table_names += [ l.strip().replace('.tsv', '') for l in f ]\n",
    "name_to_table_idx:Dict[str,int] = { n:i for i,n in enumerate(table_names) }\n",
    "table_names[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qanda = [] # Gather all question\n",
    "for fold in 'train|dev|test'.split('|'):\n",
    "    # Train set has 1 question without explanations: Mercury_7221305\n",
    "    qanda += [qa for qa in dataset.load_qanda(fold, regenerate=regenerate)\n",
    "               if fold=='test' or len(qa.explanation_gold)>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(BaseModel):\n",
    "    id:Union[str, dataset.UID]\n",
    "    is_statement:bool=False\n",
    "    is_question:bool =False; n_ans:int=0\n",
    "    is_ansY:bool     =False\n",
    "    is_ansN:bool     =False\n",
    "    raw_txt:str\n",
    "    keywords:dataset.Keywords\n",
    "    table_idx:int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_nodes:List[Node] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements_existing=set()\n",
    "for s in statements:\n",
    "    if not len(s.uid)==19: continue # Only do base statements (not combos)  FIXME\n",
    "    if not s.uid in statements_existing:\n",
    "        graph_nodes.append( Node(id=s.uid, is_statement=True,\n",
    "                                 keywords=s.keywords, raw_txt=s.raw_txt, \n",
    "                                 table_idx=name_to_table_idx[s.table], ) )\n",
    "        statements_existing.add(s.uid)\n",
    "    else:\n",
    "        print(f\"Duplicate statement ignored : {s.uid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qa in qanda:\n",
    "    graph_nodes.append( Node(id=qa.question_id, is_question=True, n_ans=len(qa.answers),\n",
    "                             keywords=qa.question.keywords, raw_txt=qa.question.raw_txt, \n",
    "                             table_idx=name_to_table_idx['Q'], ) )\n",
    "    for i,ans in enumerate(qa.answers):\n",
    "        graph_nodes.append( Node(id=f\"{qa.question_id}_A{i}\", \n",
    "                                 is_ansY=(i==0), is_ansN=(i>0),\n",
    "                                 keywords=ans.keywords, raw_txt=ans.raw_txt, \n",
    "                                 table_idx=name_to_table_idx['A-right' if i==0 else 'A-wrong'], ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups=defaultdict(int)\n",
    "for i,n in enumerate(graph_nodes):\n",
    "    dups[n.id]+=1\n",
    "[ k for k,v in dups.items() if v>1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a quick look-up from statment/question/answer id to node idx\n",
    "id_to_graph_node_idx = { n.id:i for i,n in enumerate(graph_nodes) } \n",
    "\n",
    "print(f\"{len(graph_nodes):,} == {len(id_to_graph_node_idx):,}\") # 30,856"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form a big list of keyword->node, so we can then do edges from that\n",
    "kw_to_graph_idx = defaultdict(list)\n",
    "for idx, node in enumerate(graph_nodes):\n",
    "    for kw in node.keywords:\n",
    "        kw_to_graph_idx[kw].append(idx)\n",
    "print(len(kw_to_graph_idx)) # 6527"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "', '.join(f\"{kw}={len(arr)}\" for kw, arr in kw_to_graph_idx.items() if len(arr)>500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_edges=[]\n",
    "for kw, arr in kw_to_graph_idx.items():\n",
    "    for i in arr:\n",
    "        for j in arr:\n",
    "            if i==j:continue\n",
    "            graph_edges.append( (i,j) )\n",
    "print(f\"{len(graph_edges):,}\") # 27,271,684    # 6secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate links\n",
    "graph_edges = sorted(list(set(graph_edges))) # Fixed order\n",
    "print(f\"n_edges={len(graph_edges):,}  \"+\n",
    "      f\"edge_fraction={len(graph_edges)/len(graph_nodes)/len(graph_nodes)*100.:.2f}%\")\n",
    "# n_edges=25,051,930  edge_fraction=2.63%      # 36secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding for all the raw texts\n",
    "raw_txt_arr = [ n.raw_txt for n in graph_nodes ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import blake2b\n",
    "h = blake2b(digest_size=20)\n",
    "for txt in raw_txt_arr:\n",
    "    h.update(txt.encode('utf-8'))\n",
    "raw_txt_hash = h.hexdigest()\n",
    "raw_txt_hash # '18c2325ea80990539f32ab5f97173541007ddef0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the file is there, use it, otherwise generate it...\n",
    "embedding_path=\"../data/cache/embedding\" \n",
    "raw_txt_embed_file=f\"{embedding_path}/{raw_txt_hash}.npz\"\n",
    "\n",
    "if not os.path.isfile(raw_txt_embed_file):  # Avoid loading model if possible\n",
    "    os.makedirs(embedding_path, exist_ok=True)\n",
    "    \n",
    "    model_file='distilbert-base-nli-stsb-mean-tokens'\n",
    "    \n",
    "    t0=time.time()\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    #model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "    model = SentenceTransformer(f\"../data/{model_file}\")  # <2 secs\n",
    "    t1=time.time()\n",
    "    \n",
    "    raw_txt_embed_arr = model.encode(raw_txt_arr)   #  15secs\n",
    "    t2=time.time()\n",
    "    print(f\"DONE embedding from scratch : initialisation={t1-t0:.2f}secs, embedding={t2-t1:.2f}secs\")\n",
    "    \n",
    "    np.savez(raw_txt_embed_file, emb=np.array(raw_txt_embed_arr))\n",
    "    raw_txt_embed_arr=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "pl.seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANK_MAX=512\n",
    "#BATCH_SIZE=64  # Set below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, DataLoader\n",
    "import torch_geometric.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
    "\n",
    "# This will be updated for each different question/prediction input\n",
    "#   and will be the .x values\n",
    "ranker = torch.tensor( [ [0.] for n in graph_nodes ], dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "# This is auxilliary data (embeddings, etc)\n",
    "table_idx = torch.tensor( [ [n.table_idx] for n in graph_nodes ], dtype=torch.long, requires_grad=False)\n",
    "bools     = torch.tensor( [ [n.is_statement, n.is_question, n.is_ansY, n.is_ansN, ] \n",
    "                            for n in graph_nodes ], dtype=torch.int32, requires_grad=False)\n",
    "\n",
    "#edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "#                           [1, 0, 2, 1]], dtype=torch.long)\n",
    "#edge_index = torch.tensor([\n",
    "#    [ pair[0] for pair in graph_edges],\n",
    "#    [ pair[1] for pair in graph_edges],\n",
    "#], dtype=torch.long)\n",
    "edge_index_t = torch.tensor(graph_edges, dtype=torch.long)  \n",
    "\n",
    "graph_data = Data(x=ranker, table_idx=table_idx, bools=bools, \n",
    "                  edge_index=edge_index_t.t().contiguous()) # Like suggested in the intro docs\n",
    "                  #edge_index=edge_index)\n",
    "graph_data.num_nodes, graph_data.num_edges # (30856, 25051930)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "graph_data = graph_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(np.load(raw_txt_embed_file).keys())\n",
    "raw_txt_embedding = torch.tensor(np.load(raw_txt_embed_file)['emb'], dtype=torch.float32).to(device)\n",
    "raw_txt_embedding.shape # torch.Size([30856, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RerankGraphDataset(Dataset):\n",
    "    def __init__(self, fold='dev', preds_file='../predictions/predict.FOLD.baseline-retrieval.txt',\n",
    "                ):\n",
    "        self.fold  = fold\n",
    "        \n",
    "        regenerate=False\n",
    "        # Train set has 1 question without explanations: Mercury_7221305\n",
    "        self.qanda = [qa for qa in dataset.load_qanda(fold, regenerate=regenerate)\n",
    "                         if fold=='test' or len(qa.explanation_gold)>0]\n",
    "        \n",
    "        # Load up prediction set\n",
    "        preds=defaultdict(list) # qa_id -> [statements in order]\n",
    "        with open(preds_file.replace('FOLD', self.fold), 'rt') as f:\n",
    "            for l in f.readlines():\n",
    "                qid, uid = l.strip().split('\\t')\n",
    "                #if qid not in preds: preds[qid]=[]\n",
    "                preds[qid].append(uid)\n",
    "        self.preds=preds\n",
    "        \n",
    "        self.ranker_base = np.linspace( 0.95, 0.05, num=RANK_MAX)\n",
    "        self.return_Data=True\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.qanda)\n",
    "    \n",
    "    def question_id(self, idx):\n",
    "        qa = self.qanda[idx]\n",
    "        return qa.question_id\n",
    "    \n",
    "    def __getitem__(self, idx):  # This corresponds to a specific question\n",
    "        qa = self.qanda[idx]\n",
    "        q_id=qa.question_id\n",
    "        \n",
    "        # Want to return a list of nodes in graph_data be retained\n",
    "        #   Can calculate edges using\n",
    "        #     https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.subgraph\n",
    "            \n",
    "        # And return corresponding 'x' and 'y' values for each of these slots too\n",
    "        q_node_idx = id_to_graph_node_idx[q_id]\n",
    "        n_ans = graph_nodes[q_node_idx].n_ans  # This must be a question...\n",
    "        if n_ans<=0:\n",
    "            print(f\"Question not found : {q_id}\")\n",
    "            n_ans=0\n",
    "\n",
    "        # Set up the nodes in 'condensed form'\n",
    "        n_nodes  = RANK_MAX+1+n_ans\n",
    "        node_idx = np.zeros( (n_nodes,), dtype=np.int32 )    # These are the nodes of interest\n",
    "        ranker   = np.zeros( (n_nodes,), dtype=np.float32 )  # This is the previous output\n",
    "        labels   = np.zeros( (n_nodes,), dtype=np.float32 )  # This is the {0,1} target  (BCE likes floats)\n",
    "        \n",
    "        ranker[:RANK_MAX] = self.ranker_base[:RANK_MAX]\n",
    "        \n",
    "        pred, pred_uid_to_idx=self.preds[q_id][:RANK_MAX], {}\n",
    "        for i,uid in enumerate(pred):  # For all the predictions\n",
    "            node_idx[i]= id_to_graph_node_idx[uid]\n",
    "            pred_uid_to_idx[uid] = i # Needed for explanation_gold population of labels\n",
    "        #pred_uid_to_idx = { uid:i for i, uid in enumerate(pred) }  # Needed for explanation_gold\n",
    "        \n",
    "        # Here's the question\n",
    "        node_idx[RANK_MAX] = q_node_idx\n",
    "        # And the answers\n",
    "        for i in range(n_ans):\n",
    "            node_idx[RANK_MAX+1+i] = id_to_graph_node_idx[f\"{q_id}_A{i}\"]\n",
    "        \n",
    "        for ex in qa.explanation_gold:\n",
    "            if ex.uid in pred_uid_to_idx:  # These are in same positions as node_idx\n",
    "                labels[ pred_uid_to_idx[ex.uid] ] = 1.\n",
    "            else:\n",
    "                pass\n",
    "                #print(f\"Missing explanation: {ex.uid}\")\n",
    "        #if labels.sum()==0.: labels[0]=1. # Prevent NAN if nothing is 1 for APLoss...\n",
    "        \n",
    "        if not self.return_Data:\n",
    "            return dict( idx=idx, q_id=qa.question_id,\n",
    "                            node_idx = node_idx, \n",
    "                            ranker = ranker,\n",
    "                            labels = labels,\n",
    "                        )\n",
    "        subset = torch.tensor(node_idx, dtype=torch.long)\n",
    "        edge_index_subset, _ = torch_geometric.utils.subgraph(subset, graph_data.edge_index, \n",
    "                                                              relabel_nodes=True)\n",
    "        ranker    = torch.tensor(ranker, requires_grad=False)\n",
    "        #table_idx = graph_data.table_idx[subset]  # This becomes 'relabelled' in the same way\n",
    "        #bools     = graph_data.bools[subset]      # This becomes 'relabelled' in the same way\n",
    "\n",
    "        labels    = torch.tensor(labels, requires_grad=False)\n",
    "\n",
    "        return Data(x=ranker, \n",
    "                    #table_idx=table_idx, bools=bools, \n",
    "                    subset_indices=subset,\n",
    "                    edge_index=edge_index_subset, \n",
    "                    y=labels)\n",
    "\n",
    "ds_dev  =RerankGraphDataset(fold='dev')\n",
    "ds_train=RerankGraphDataset(fold='train')\n",
    "ds_test =RerankGraphDataset(fold='test')\n",
    "#ds_dev[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RerankGraphDataset_to_cache(ds, fold, regenerate=False):\n",
    "    folder=f\"../data/cache/{fold}\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    def as_numpy(p): return p.detach().cpu().numpy()\n",
    "    for idx in range(len(ds)):\n",
    "        cache_file = os.path.join(folder, f\"{idx:05d}.npz\")\n",
    "        if (idx+1)%100==0: print(cache_file)\n",
    "        if os.path.isfile(cache_file) and not regenerate:\n",
    "            continue # Skip this, it's there already\n",
    "        data = ds[idx]\n",
    "        np.savez(cache_file, \n",
    "                 x=as_numpy(data.x), \n",
    "                 subset_indices=as_numpy(data.subset_indices),\n",
    "                 edge_index=as_numpy(data.edge_index),\n",
    "                 y=as_numpy(data.y),\n",
    "        )\n",
    "    print(f\"DONE : {fold} - {cache_file}\")\n",
    "\n",
    "regenerate=False\n",
    "RerankGraphDataset_to_cache(ds_train, 'train', regenerate=regenerate) # 22x 30sec\n",
    "RerankGraphDataset_to_cache(ds_dev,   'dev'  , regenerate=regenerate) # 5x  30sec\n",
    "RerankGraphDataset_to_cache(ds_test,  'test' , regenerate=regenerate) # 16x 30sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RerankGraphDatasetFromCache(Dataset):\n",
    "    def __init__(self, fold='dev'):\n",
    "        self.folder=f\"../data/cache/{fold}\"\n",
    "        self.files=sorted(f for f in os.listdir(self.folder) if f.endswith('.npz'))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):  # This corresponds to a specific question\n",
    "        cache_file = os.path.join(self.folder, self.files[idx])\n",
    "        data = np.load(cache_file)\n",
    "        return Data(x=torch.tensor(data['x'], dtype=torch.float),  # , requires_grad=False\n",
    "                    subset_indices=torch.tensor(data['subset_indices'], dtype=torch.long),\n",
    "                    edge_index=torch.tensor(data['edge_index'], dtype=torch.long),\n",
    "                    y=torch.tensor(data['y'], dtype=torch.float),\n",
    "                )\n",
    "\n",
    "ds_train_cached = RerankGraphDatasetFromCache(fold='train')\n",
    "ds_dev_cached   = RerankGraphDatasetFromCache(fold='dev')\n",
    "ds_test_cached  = RerankGraphDatasetFromCache(fold='test')\n",
    "\n",
    "len(ds_train_cached), len(ds_dev_cached), len(ds_test_cached) # (2206, 496, 1664)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_cached[1] # Data(edge_index=[2, 50730], subset_indices=[517], x=[517], y=[517])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=413\n",
    "ds_dev.return_Data=False\n",
    "d_plain=ds_dev[idx]\n",
    "\n",
    "ds_dev.return_Data=True\n",
    "d_data =ds_dev[idx]\n",
    "\n",
    "# 0.7083 0.7083 1.0000 : XX...X.....X.................................................... : \n",
    "#   # 413 = Mercury_7264023 :: [ CONTAINS.CENTRAL KINDOF.GROUNDING MADEOF.NEG SYNONYMY.LEXGLUE ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This uses just the 'plain' data\n",
    "print(d_plain['node_idx'].shape)\n",
    "\n",
    "for idx in [1684, 4411, 5149, 7828,   # These are the gold explanation statements \n",
    "            22123, 22124, 22125, 22126, 22127  # These are the question + answer nodes\n",
    "           ]:\n",
    "    uid=graph_nodes[idx].id\n",
    "    table_idx = graph_nodes[idx].table_idx\n",
    "    print(f\"{list(d_plain['node_idx']).index(idx):3d} {uid:>25s} : {table_idx:2d}={table_names[table_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subgraph(d): # This is 'outside' the main ds_data - for testing/debugging\n",
    "    subset = torch.tensor(d['node_idx'], dtype=torch.long).to(device)\n",
    "    edge_index_subset, _ = torch_geometric.utils.subgraph(subset, graph_data.edge_index, relabel_nodes=True)\n",
    "    #print(edge_index.type(), edge_index.shape ) # torch.cuda.LongTensor torch.Size([2, 39206])\n",
    "    \n",
    "    ranker    = torch.tensor(d['ranker'], requires_grad=False).to(device)\n",
    "    table_idx = graph_data.table_idx[subset]  # This becomes 'relabelled' in the same way\n",
    "    bools     = graph_data.bools[subset]      # This becomes 'relabelled' in the same way\n",
    "\n",
    "    labels    = torch.tensor(d['labels'], requires_grad=False).to(device)\n",
    "\n",
    "    return Data(x=ranker, \n",
    "                table_idx=table_idx, bools=bools,  # This is 'unwrapped' representation\n",
    "                subset_indices=subset,             # This is for deferred unwrapping\n",
    "                edge_index=edge_index_subset, \n",
    "                y=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph = make_subgraph(d_plain)\n",
    "for idx in [0,1,5,11,   # These are the gold explanation statements \n",
    "            512, 513, 514, 515, 516  # These are the question + answer nodes\n",
    "           ]:\n",
    "    print(f\"{idx:3d} -> {d_plain['node_idx'][idx]:5d}, \" # Agrees with above\n",
    "          +f\"table_idx={subgraph['table_idx'][idx].item():2d}, \"\n",
    "          #+f\"bools={subgraph['bools'][idx]}\"\n",
    "          +f\"bools={list(subgraph['bools'][idx].cpu().numpy())}\"\n",
    "         )\n",
    "    \n",
    "    table_idx=graph_data.table_idx[subgraph.subset_indices]\n",
    "    bools    =graph_data.bools[subgraph.subset_indices]\n",
    "    print(f\"{' '*21}{table_idx[idx].item():5d}, bools={list(bools[idx].cpu().numpy())}\")\n",
    "    #print(f\"{' '*21}{d_data.table_idx[idx].item():5d}, bools={list(d_data.bools[idx].cpu().numpy())}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index_subgraph_list = list(subgraph.edge_index.t().cpu().numpy())\n",
    "print(sorted([ list(pair) \n",
    "               for pair in edge_index_subgraph_list \n",
    "               if pair[0]==5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density of edges within this subgraph\n",
    "subgraph.edge_index, subgraph.edge_index.shape[1]/(subgraph.subset_indices.shape[0]**2) * 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn.conv import GATConv, GatedGraphConv\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        table_emb_size, hidden_size=8, 64\n",
    "        \n",
    "        self.table_embedding = torch.nn.Embedding(\n",
    "            len(table_names), table_emb_size, \n",
    "            max_norm=1.0,\n",
    "        ) # , requires_grad=True is the default::\n",
    "        \n",
    "        # ranker, table.embedding, bools\n",
    "        n_features = 1+table_emb_size+4\n",
    "        \n",
    "        #self.conv1 = GCNConv(n_features, hidden_size)\n",
    "        #self.conv2 = GCNConv(hidden_size, 1)\n",
    "        \n",
    "        # As long as n_features<hidden_size, this will just be padded out...\n",
    "        self.gru = GatedGraphConv(hidden_size, num_layers=4, aggr='max')\n",
    "\n",
    "        self.att_conv1 = GATConv(hidden_size, hidden_size//8, heads=8, )  # dropout=0.6\n",
    "        #self.att_conv2 = GATConv(hidden_size, 1, heads=1, concat=False, ) # dropout=0.6\n",
    "        \n",
    "        self.final  = torch.nn.Linear(hidden_size, 1)\n",
    "        self.output = torch.nn.Sigmoid()\n",
    "        \n",
    "        self.bx_loss = torch.nn.BCELoss()\n",
    "\n",
    "    def forward(self, data):\n",
    "        subset = data.subset_indices   # This is sent over 'per qa'\n",
    "        ranker = data.x                # This is sent over 'per qa'\n",
    "        ranker = ranker*4.0-2.0        # Scale to be the approx same range as the LSTM one (+/-2)\n",
    "        \n",
    "        table_idx = graph_data.table_idx[subset]  # This uses the on-device full graph\n",
    "        bools     = graph_data.bools[subset]      # This uses the on-device full graph\n",
    "\n",
    "        edge_index = data.edge_index\n",
    "        \n",
    "        #print(f\"ranker.size() : {ranker.size()}\")        # torch.Size([517])\n",
    "        #print(f\"table_idx.size() : {table_idx.size()}\")  # table_idx.size() : torch.Size([517, 1])\n",
    "        #print(f\"bools.size() : {bools.size()}\")          # bools.size() : torch.Size([517, 4])\n",
    "\n",
    "        x_table_emb = self.table_embedding(table_idx)\n",
    "        #print(f\"x_table_emb.size() : {x_table_emb.size()}\")  # x_table_emb.size() : torch.Size([517, 1, 8])\n",
    "\n",
    "        # raw_txt_embedding\n",
    "        \n",
    "        x = torch.cat( [ranker.unsqueeze(-1), x_table_emb.squeeze(1), bools.float() ], axis=-1)\n",
    "\n",
    "        if False:\n",
    "            for idx in [0,1,256,512,513,514]:\n",
    "                print(idx, x[idx,:])\n",
    "        \n",
    "        #x = self.conv1(x, edge_index)\n",
    "        #x = F.relu(x)\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        #x = self.conv2(x, edge_index)\n",
    "\n",
    "        x = self.gru(x, edge_index)\n",
    "        \n",
    "        x = F.elu(self.att_conv1(x, edge_index))\n",
    "        #x = F.dropout(x, p=0.6, training=self.training)\n",
    "        #x = self.conv2(x, edge_index)\n",
    "        \n",
    "        #print(f\"x.size() : {x.size()}\")          # x.size() : torch.Size([517, 64])\n",
    "        x = self.final(x)\n",
    "        #print(f\"final.size() : {x.size()}\")      # final.size() : torch.Size([517, 1])\n",
    "        \n",
    "        x = x+ranker.unsqueeze(-1)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01 ) #, weight_decay=5e-4)\n",
    "#list(model.parameters())\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "#loader_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True) #, num_workers=4)\n",
    "##loader_dev   = DataLoader(ds_dev, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "loader_train = DataLoader(ds_train_cached, batch_size=BATCH_SIZE, shuffle=True) #, num_workers=4)\n",
    "loader_dev   = DataLoader(ds_dev_cached, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def run_training_OLD(d, steps=10):\n",
    "    subgraph, edge_index_subgraph = make_subgraph(d)\n",
    "    for step in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        #ranks_pred = model(ranker, table_idx, bools, edge_index)\n",
    "        ranks_pred = model(subgraph, edge_index_subgraph)\n",
    "        \n",
    "        #loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss = model.bx_loss(ranks_pred, subgraph.y.unsqueeze(-1))\n",
    "        #print(i, loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        clipping_value = 1. # arbitrary value of your choosing\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping_value)\n",
    "        \n",
    "        optimizer.step()\n",
    "    #break\n",
    "    return loss.item()  # Final loss\n",
    "\n",
    "def run_training(epoch, loader, loader_len, steps=1):\n",
    "    loss_all=0.\n",
    "    for data in tqdm(loader, total=loader_len, desc=f\"epoch[{epoch}]\"):\n",
    "        #print(data.device)\n",
    "        data = data.to(device)    \n",
    "        for step in range(steps):\n",
    "            optimizer.zero_grad()\n",
    "            ranks_pred = model(data)\n",
    "\n",
    "            loss = model.bx_loss(ranks_pred, data.y.unsqueeze(-1))\n",
    "            loss.backward()\n",
    "\n",
    "            #print(f\"{loss.item():.4f}\")\n",
    "            clipping_value = 1. # arbitrary value of your choosing\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clipping_value)\n",
    "            \n",
    "            optimizer.step()\n",
    "        loss_all += loss.item() * data.num_graphs  # Just at the last step\n",
    "    return loss_all  # total loss\n",
    "\n",
    "def run_validation_OLD(d):\n",
    "    subgraph, edge_index_subgraph = make_subgraph(d)\n",
    "    with torch.no_grad():\n",
    "        ranks_pred = model(subgraph, edge_index_subgraph)\n",
    "        loss = model.bx_loss(ranks_pred, subgraph.y.unsqueeze(-1))\n",
    "    return loss.item()\n",
    "\n",
    "def run_validation(epoch, loader, loader_len):\n",
    "    loss_all=0.\n",
    "    for data in tqdm(loader, total=loader_len, desc=f\"epoch[{epoch}].dev\", leave=None):\n",
    "        #print(data.device)\n",
    "        data = data.to(device)    \n",
    "        with torch.no_grad():\n",
    "            data = data.to(device)    \n",
    "            ranks_pred = model(data)\n",
    "            loss = model.bx_loss(ranks_pred, data.y.unsqueeze(-1))\n",
    "        loss_all += loss.item() * data.num_graphs  # Just at the last step\n",
    "    return loss_all  # total loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps=1\n",
    "for i in []: # range(50):\n",
    "    #loss=run_training(d, steps=n_steps)\n",
    "    loss=run_validation(d_data)\n",
    "    print(f\"step[{i*n_steps:5d}] : loss_bx={loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-state-dict-recommended\n",
    "from datetime import datetime\n",
    "def save_model(model, epoch, loss, stub=\"model\", fmt=\"./TS_MODEL_EPOCH_LOSS.pth\"):\n",
    "    filename = (fmt\n",
    "                 .replace(\"TS\", datetime.now().strftime('%Y-%m-%dT%H.%M.%S%z'))\n",
    "                 .replace(\"MODEL\", stub)\n",
    "                 .replace(\"EPOCH\", f\"{epoch:03d}\")\n",
    "                 .replace(\"LOSS\",  f\"{loss:.4f}\")\n",
    "               )\n",
    "    torch.save(model.state_dict(), filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train_len = np.ceil(len(ds_train)/BATCH_SIZE)\n",
    "loader_dev_len   = np.ceil(len(ds_dev)  /BATCH_SIZE)\n",
    "\n",
    "model_name='graph-with-tables'\n",
    "\n",
    "loss_best=999.\n",
    "for epoch in range(50): # []:\n",
    "    loss_tot=run_training(epoch, loader_train, loader_train_len, steps=4)\n",
    "    loss = loss_tot/len(ds_train)\n",
    "    \n",
    "    loss_tot = run_validation(epoch, loader_dev, loader_dev_len)\n",
    "    loss_dev = loss_tot/len(ds_dev)\n",
    "    print(f\"epoch[{epoch}] : train.loss_bx.mean()={loss:.4f}, dev.loss_bx.mean()={loss_dev:.4f}\")\n",
    "    \n",
    "    if loss_best>loss_dev:\n",
    "        loss_best=loss_dev\n",
    "        save_model(model, epoch, loss_dev, stub=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BX target from LSTM list reranker : ~0.0360 (dev-set)\n",
    "#   Here : \n",
    "#     epoch[28] : train.loss_bx.mean()=0.0338, dev.loss_bx.mean()=0.0347\n",
    "best_model=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_model(model, epoch, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = Net()\n",
    "#best_model.load_state_dict(torch.load('2020-09-21T02.25.19_026_0.0354.pth'))\n",
    "#best_model.load_state_dict(torch.load('2020-09-21T22.35.23_023_0.0361.pth'))\n",
    "\n",
    "#best_model.load_state_dict(torch.load('2020-09-22T00.42.12_MODEL_028_0.0347.pth'))  # +/- 2ish\n",
    "best_model.load_state_dict(torch.load('2020-09-22T00.52.37_MODEL_037_0.0345.pth'))  # +/- 2ish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = best_model.to(device)\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reranked(ds, ds_cached, fold='dev', \n",
    "                  preds_file='../predictions/predict.FOLD.baseline-retrieval_plus-graph.txt'):\n",
    "    with open(preds_file.replace('FOLD', fold), 'wt') as f:\n",
    "        for idx, data in enumerate(ds_cached):\n",
    "            q_id = ds.question_id(idx)\n",
    "            print(q_id)\n",
    "            with torch.no_grad():\n",
    "                data = data.to(device)\n",
    "                #print(data.x.device, data.subset_indices.device, ) # cuda:0 cuda:0\n",
    "                #print(model.table_emb.device)\n",
    "                ranks_pred = best_model(data)\n",
    "            preds_np = ranks_pred.detach().cpu().numpy()\n",
    "            print(preds_np.shape)\n",
    "            preds_np = preds_np[:RANK_MAX, 0]\n",
    "            print(preds_np[:20])\n",
    "            ranks_np=np.argsort(-preds_np)  # This is the order of the ids we should output\n",
    "            print(ranks_np[:20])\n",
    "\n",
    "            preds_original = ds.preds[q_id]\n",
    "\n",
    "            reranked = [preds_original[idx] for idx in list(ranks_np)]\n",
    "            reranked = reranked + preds_original[RANK_MAX:]\n",
    "            #print(len(preds_original), len(reranked))\n",
    "            print(preds_original[:10]);print(reranked[:10])\n",
    "            #if q_id=='MCAS_2009_8_18':break\n",
    "            for p in reranked:\n",
    "                f.write(f\"{q_id}\\t{p}\\n\")\n",
    "            \n",
    "                \n",
    "save_reranked(ds_dev, ds_dev_cached, fold='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MCAS_2009_8_18\n",
    "#(517, 1)\n",
    "#[ 0.22 0.13 0.10 0.15 0.07 0.04 0.11 0.16 0.16 0.21 0.02 0.09 0.13 0.05 0.07 0.09 0.10 0.06 0.06 0.02]\n",
    "#[22 29  0  9 44  8  7  3  1 12  6 35 43 67 16  2 27 11 15 32]\n",
    "#['357e-a596-31bf-15a2', 'ae9d-5e74-afa3-d031', '6c7e-2cf1-8a3f-b14a', '73e5-00b0-22fd-e34e', 'f685-05f4-46b2-1a83', '7984-5745-044b-f0ab', 'cb9b-7410-93c7-2adf', 'd3b3-ecd0-5b4d-bedf', '0f8b-28e6-6305-a476', 'a0b2-a45f-01e1-4bf4']\n",
    "#['09c2-3e04-4343-223d', '3f2b-87bb-f73a-3e5d', '357e-a596-31bf-15a2', 'a0b2-a45f-01e1-4bf4', '81b7-0a58-5b0b-0bbf', '0f8b-28e6-6305-a476', 'd3b3-ecd0-5b4d-bedf', '73e5-00b0-22fd-e34e', 'ae9d-5e74-afa3-d031', 'a10a-a44b-4ab6-225f']\n",
    "\n",
    "#MCAS_2009_8_18\n",
    "#(517, 1)\n",
    "#[ 0.28 0.19 0.13 0.24 0.24 0.07 0.21 0.18 0.17 0.12 0.03 0.08 0.16 0.04 0.03 0.06 0.17 0.03 0.05 0.02]\n",
    "#[22  0  4  3  6  1  7 16  8 12  2  9 11 35  5 44 15 21 23 67]\n",
    "#['357e-a596-31bf-15a2', 'ae9d-5e74-afa3-d031', '6c7e-2cf1-8a3f-b14a', '73e5-00b0-22fd-e34e', 'f685-05f4-46b2-1a83', '7984-5745-044b-f0ab', 'cb9b-7410-93c7-2adf', 'd3b3-ecd0-5b4d-bedf', '0f8b-28e6-6305-a476', 'a0b2-a45f-01e1-4bf4']\n",
    "#['09c2-3e04-4343-223d', '357e-a596-31bf-15a2', 'f685-05f4-46b2-1a83', '73e5-00b0-22fd-e34e', 'cb9b-7410-93c7-2adf', 'ae9d-5e74-afa3-d031', 'd3b3-ecd0-5b4d-bedf', 'bb18-f19f-69ba-8bc6', '0f8b-28e6-6305-a476', 'a10a-a44b-4ab6-225f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../tg2020task/evaluate.py --gold ../tg2020task/questions.dev.tsv \\\n",
    "                                  ../predictions/predict.dev.baseline-retrieval_plus-graph.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP:  0.4196867493738269 # !! WTF?\n",
    "# MAP:  0.4613060121071838 # Still WTF... (with +/-2 ranker re-scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
