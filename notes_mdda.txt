*  https://sites.google.com/view/textgraphs2021
  *  http://cognitiveai.org/explanationbank/
  *  https://github.com/cognitiveailab/tg2021task

*  https://competitions.codalab.org/competitions/29228


## Important dates : 

Important Dates
2021-02-15: Training data release
# NOW
2021-03-10: Test data release; Evaluation start

2021-03-24: Evaluation end :: March 24, 2021, midnight UTC according to codalab


2021-04-01: System description paper deadline
2021-04-13: Deadline for reviews of system description papers
2021-04-15: Author notifications
2021-04-26: Camera-ready description paper deadline
2021-06-11: TextGraphs-15 workshop

To encourage transparency and replicability, all teams must publish their : 
  code, 
  tuning procedures, and 
  instructions for running their models with their submission of shared task papers.



*   Train = 2207 examples
*   Dev   =  496 examples
*   Test  = 1664 examples

## Scores

1 DeepBlueAI 11	03/23/21	DeepBlueAI	  0.7285  (later)
1 yuejiaxiang	1	03/19/21	google-BERT		0.7003
2 iiacobac		4	03/12/21	huawei_noah		0.6493
3 dustalov		2	03/10/21					      0.5010




https://aideadlin.es/?sub=NLP








## Installation

git clone https://github.com/mdda/worldtree_corpus.git
cd worldtree_corpus  # i.e. the REPO root directory
#git branch -a
git checkout -b textgraphs_2021 origin/textgraphs_2021

./run_setup.bash
# ./run_baseline.bash



# Do the train split on 2210 Questions : Takes ~420secs
./baseline_tfidf.py data-evalperiod/tables data-evalperiod/wt-expert-ratings.train.json > predict.baseline_tfidf.train.txt
# ./evaluate.py --gold data/wt-expert-ratings.dev.json predict.txt
./evaluate.py --gold data-evalperiod/wt-expert-ratings.train.json predict.baseline_tfidf.train.txt
# Mean NDCG Score : 0.5167861075889512



# Do the dev split on 496 Questions : Takes ~100secs
./baseline_tfidf.py data-evalperiod/tables data-evalperiod/wt-expert-ratings.dev.json > predict.baseline_tfidf.dev.txt
# ./evaluate.py --gold data/wt-expert-ratings.dev.json predict.txt
./evaluate.py --gold data-evalperiod/wt-expert-ratings.dev.json predict.baseline_tfidf.dev.txt
# Mean NDCG Score : 0.513046779054998  (agrees with 0.5130 NDCG given in the GitHub README.md)


# Do the text split on 1670 Questions # Actually :: Cannot, since the gold results are unknown to us
./baseline_tfidf.py data-evalperiod/tables data-evalperiod/wt-expert-ratings.test.json > predict.baseline_tfidf.test.txt
#./evaluate.py --gold data-evalperiod/wt-expert-ratings.test.json predict.baseline_tfidf.test.txt
# Guess at site evaluation on test set : 0.5010 (dustalov)
#   THEREFORE : The expected_test_score = train_score-0.0160 (don't rely on this)
#   THEREFORE : The expected_test_score =   dev_score-0.0120 (could be decent estimate)



cd ../src
python baseline_retrieval.py  # Saves to ../predictions/predict.dev.baseline-retrieval.txt

# Basic retrieval (before hyperopt)
cd ../tg2021task/
./evaluate.py --gold data-evalperiod/wt-expert-ratings.dev.json ../predictions/predict.dev.baseline-retrieval.txt
Mean NDCG Score : 0.6669


# After hyperopt
cd ../tg2021task/
./evaluate.py --gold data-evalperiod/wt-expert-ratings.dev.json ../predictions/predict.dev.baseline-retrieval.txt
Mean NDCG Score : 0.6785199870115878


python3 -m pip install --upgrade nni






# train.baseline.txt => 0.2469 MAP locally
# dev.baseline.txt   => 0.2550 MAP locally
# test.baseline.txt  => 0.2347 MAP (Test uploaded on 2020-08-31)

## ...

*  Need a lemmatizer to correctly resolve all 'node words' in the explanations
   *  Good resource : https://www.machinelearningplus.com/nlp/lemmatization-examples-python/
   
*  Can use this to identify nodes in Q&s 


git config --global credential.helper cache
git config --global credential.helper 'cache --timeout=36000'  # 10hrs




https://github.com/VHRanger/nodevectors
https://github.com/benedekrozemberczki/KarateClub
https://networkx.github.io/documentation/stable/reference/algorithms/shortest_paths.html


Applying Graph Neural Networks on Heterogeneous Nodes and Edge Features
  https://grlearning.github.io/papers/6.pdf


https://twitter.com/PetarV_93/status/1306689702020382720

https://petar-v.com/GAT/

  Gated Attention Networks (GaAN) (Zhang et al., 2018), 
    where gating mechanisms are inserted into the multi-head attention system of GATs, 
    in order to give different value to different headsâ€™ computations. 
    Several challenging baselines are outperformed on both 
    inductive node classification tasks (Reddit, PPI) and 
    a traffic speed forecasting task (METR-LA).


GRAPH CONVOLUTIONAL NETWORKS
  THOMAS KIPF, 30 SEPTEMBER 2016
    https://tkipf.github.io/graph-convolutional-networks/


CUDA=cu102
pip install torch-scatter==latest+${CUDA} -f https://pytorch-geometric.com/whl/torch-1.6.0.html
pip install torch-sparse==latest+${CUDA} -f https://pytorch-geometric.com/whl/torch-1.6.0.html
pip install torch-cluster==latest+${CUDA} -f https://pytorch-geometric.com/whl/torch-1.6.0.html
pip install torch-spline-conv==latest+${CUDA} -f https://pytorch-geometric.com/whl/torch-1.6.0.html
pip install torch-geometric

5095-dfd3-1847-a4a0 : TWICE in RELATIONSHIP.tsv
9b87-dd15-0cc5-32aa : TWICE in OPPOSITES.tsv
5689-a3ff-212f-560a : TWICE in OPPOSITES.tsv
b69d-9d08-0ad6-3023 : TWICE in UNIT.tsv

https://www.sbert.net/docs/usage/computing_sentence_embeddings.html





https://cloud.google.com/ai-platform/deep-learning-vm/docs/cloud-marketplace

asia-southeast1-a
asia-southeast1-b	Jurong West, Singapore, APAC	E2, N2, N2D, N1, M1, C2	Ivy Bridge, Sandy Bridge, Haswell, Broadwell, Skylake, Cascade Lake, AMD EPYC Rome	GPUs
asia-southeast1-c	Jurong West, Singapore, APAC	E2, N2, N2D, N1, M1, C2, A2	Ivy Bridge, Sandy Bridge, Haswell, Broadwell, Skylake, Cascade Lake, AMD EPYC Rome	GPUs

... only T4s...

us-west1-a
16vCPU 60Gb Mem
1 V100

Want access to V100 or A100 for research purposes : Have already been given Google credits under the GDE program to do so.  
Based in Singapore, but the instances there only seem to have T4s available : So, expanding search to find a close-ish alternative machine.

Rejected

I wanted a GPU quota increase from 0 to >0.  I am a Google Developer Expert for ML (GDE ML), which is a Google program, and co-organise the TensorFlow group in Singapore with more than 4000 members.  
I have credits generously granted to me by Google to conduct a research task.  It is mind-boggling to see that GCP is rejecting my request.







ssh andrewsm@simlim
cd /mnt/data/secure/mdda/rdai
git clone https://github.com/mdda/worldtree_corpus.git
cd worldtree_corpus  # i.e. the REPO root directory
#git branch -a
git checkout -b textgraphs_2021 origin/textgraphs_2021

./run_setup.bash
ln -s ../tg2021task/data-evalperiod/wt-expert-ratings.train.json data/wt-expert-ratings.train.json
ln -s ../tg2021task/data-evalperiod/wt-expert-ratings.dev.json data/wt-expert-ratings.dev.json
ln -s ../tg2021task/data-evalperiod/wt-expert-ratings.test.json data/wt-expert-ratings.test.json
ln -s ../tg2021task/data-evalperiod/tables data/tables

. ./env38/bin/activate
python baseline_bert.py
# 99% GPU utilisation, 4777Mb GPU RAM used : 8min30 per epoch, 5 epochs => 50mins for training
0.0675
0.0459
0.0341
0.0267
0.0215
# Dev output directly...  4mins


# On local machine:
#rsync -avz --progress predictions/predict.both.baseline-retrieval.hyperopt.tar.gz andrewsm@simlim:/mnt/data/secure/mdda/rdai/worldtree_corpus/predictions/

pushd predictions
tar -xzf predict.both.baseline-retrieval.hyperopt.tar.gz
popd

python model_mdda.py  # Use defaults - initial download : 440Mb
# 98% utilisation : 5559Mb of GPU RAM used, each epoch ~20mins
0.0677
0.0404
0.0372
0.0302
0.0275
0.0190

python model_mdda.py --load lightning_logs/version_2/checkpoints/epoch=3-step=15659.ckpt
# Takes ~8mins on validation set
ndcg:        0.7409
oracle ndcg: 0.9378
# Still works(!)
# Still works(!) at 00:27 of 2021-03-24  !! Awesome


#Vivek recollection: DistilBERT:  0.72
#Vivek recollection: RegularBERT: 0.73

python model_mdda.py --bert distilbert-base-uncased
# 99% utilisation : 3475Mb of GPU RAM used, each epoch ~12mins
prediction ndcg: 0.7073
oracle ndcg    : 0.9378

python model_mdda.py --bert distilbert-base-uncased --load lightning_logs/version_7/checkpoints/epoch=0-step=3914.ckpt
prediction ndcg: 0.6790  # Have to add in as model param
oracle ndcg    : 0.9378

# Test hparam saving
python model_mdda.py --bert distilbert-base-uncased
python model_mdda.py --load lightning_logs/version_9/checkpoints/epoch=0-step=3914.ckpt
prediction ndcg: 0.6790  # Hmm - same
oracle ndcg    : 0.9378

# Test hparam saving
python model_mdda.py --bert distilbert-base-uncased
prediction ndcg: 0.7152 # At end of training
oracle ndcg    : 0.9378
python model_mdda.py --load lightning_logs/version_13/checkpoints/epoch=1-step=7829.ckpt
prediction ndcg: 0.6985 # Saved model ??
oracle ndcg    : 0.9378



# Big training with ndcg calculated continuously too
#python model_mdda.py --loss_style 2 --num_labels 7 --bert distilbert-base-uncased 
#
#python model_mdda.py --loss_style 2 --num_labels 7
#prediction ndcg: 0.6431  # before training?
#prediction ndcg: 0.8236  # loss.train=1.02
#prediction ndcg: 0.8525  # loss.train=0.825
#prediction ndcg: 0.8589  # loss.train=0.752
#prediction ndcg: 0.8516  # loss.train=0.596
#prediction ndcg: 0.8653  # loss.train=0.593
#prediction ndcg: 0.7131 (dev retrieval)

python model_mdda.py --loss_style 2 --num_labels 7 --load lightning_logs/version_20/checkpoints/epoch=4-step=19574.ckpt
prediction ndcg: 0.7131  # As expected (disappointingly)
oracle ndcg    : 0.9378

python model_mdda.py --loss_style 2 --num_labels 7 --load lightning_logs/version_20/checkpoints/epoch=4-step=19574.ckpt --fold test


# Try again with explicit model specification
python model_mdda.py --loss_style 2 --num_labels 7 --base distilbert
0.5796  # Before training
0.8278  # loss=1.02
0.8250  # loss=0.84
0.8411  # loss=0.709
0.8464  # loss=0.663
0.8486  # loss=0.428
0.6783 / 0.9378  # Final test

python model_mdda.py --load lightning_logs/version_24/checkpoints/epoch=4-step=19574.ckpt
0.6783 / 0.9378  # Final test matches perfectly...
0.6783 / 0.9378  # Final test matches perfectly...  And also with a different seed == good confirmation


# Older checkpoint still works (this is pure regression-style)
python model_mdda.py --load lightning_logs/version_2/checkpoints/epoch=3-step=15659.ckpt
prediction ndcg: 0.7409
oracle ndcg    : 0.9378


git fetch origin textgraphs_2021
From https://github.com/mdda/worldtree_corpus
 * branch            textgraphs_2021 -> FETCH_HEAD
git diff FETCH_HEAD


# Try again with explicit model specification (version_28)
python model_mdda.py --loss_style 2 --num_labels 7 --base bert
0.7489
0.7826 # loss=0.957
0.8499 # loss=0.908
0.8466 # loss=0.715
0.8553 # loss=0.623
0.8486 # loss=0.606
0.6969 # !! Final

# Fix hurdle to 0.5 on torch.sigmoid of prob_logit (previous, no sigmoid == bug)
#python model_mdda.py --load lightning_logs/version_28/checkpoints/epoch=3-step=15659.ckpt
#oracle ndcg    : 0.9378
#prediction ndcg: 0. hurdle=0.25
#prediction ndcg: 0.7075 hurdle=0.50



# Scores for submissions:
predict.dev.baseline-retrieval.hyperopt.txt:  	0.6785199870115878
predict.dev.modelversion9.txt:                  0.76796  

predict.test.baseline-retrieval.hyperopt.txt:  	0.658327  (#i.e dev-2.0)
predict.test.modelversion9.txt:                 0.75806   (#i.e dev-1.0)



### ARGH : loss_style was not being stored...
# Try again :
python model_mdda.py --loss_style 2 --num_labels 7 --base distilbert
python model_mdda.py --load lightning_logs/version_42/checkpoints/epoch\=2-step\=11744.ckpt 




"""
torch.nn.Softmax(-1)( torch.tensor([1.,2,3,5,6]) )

torch.nn.CrossEntropyLoss(
                                weight=self.output_weight, 
                                reduce=False,
                            )

torch.nn.CrossEntropyLoss()( torch.tensor([1.,2,3,5,6]).unsqueeze(0), torch.tensor([3]) )
weight=torch.tensor([0.,1,1,1,1,1])
torch.nn.CrossEntropyLoss(weight=weight)( torch.tensor([0., 1,2,3,5,6]).unsqueeze(0), torch.tensor([4]) )

torch.nn.CrossEntropyLoss(ignore_index=0)( torch.tensor([0.,1.,2,3,5,6]).unsqueeze(0), torch.tensor([3]) )
# == tensor(3.3682)
torch.nn.CrossEntropyLoss(ignore_index=-1)( torch.tensor([0.,1.,2,3,5,6]).unsqueeze(0), torch.tensor([-1]) )
# == 0

# Figure out what a uniform prior 'costs'
torch.nn.CrossEntropyLoss(ignore_index=-1)( torch.tensor([1.,1.,1,1,1,1]).unsqueeze(0), torch.tensor([3]) )
# tensor(1.7918)
torch.nn.CrossEntropyLoss(ignore_index=-1)( torch.tensor([0.,0,0,0,0,0]).unsqueeze(0), torch.tensor([-1]) )
# ==0
"""
