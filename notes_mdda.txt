*  https://sites.google.com/view/textgraphs2021
  *  http://cognitiveai.org/explanationbank/
  *  https://github.com/cognitiveailab/tg2021task

*  https://competitions.codalab.org/competitions/29228


## Important dates : 

Important Dates
2021-02-15: Training data release
# NOW
2021-03-10: Test data release; Evaluation start

2021-03-24: Evaluation end :: March 24, 2021, midnight UTC according to codalab


2021-04-01: System description paper deadline
2021-04-13: Deadline for reviews of system description papers
2021-04-15: Author notifications
2021-04-26: Camera-ready description paper deadline
2021-06-11: TextGraphs-15 workshop

To encourage transparency and replicability, all teams must publish their : 
  code, 
  tuning procedures, and 
  instructions for running their models with their submission of shared task papers.



*   Train = 2207 examples
*   Dev   =  496 examples
*   Test  = 1664 examples

## Scores

(2020-09-02)
  Username    Score   
1	alvysinger  0.5815
2	webbley     0.5809
3	aisys       0.5233

(2020-09-19)
  Username    Score   
1	webbley     0.5831 (increased)
2	alvysinger  0.5815 (same)
3	aisys       0.5233 (same)

(2020-09-21)
#	Username	Score
1	webbley     0.6033 (increased)
2	alvysinger  0.5815 (same)
3	aisys       0.5233 (same)

6	redken	3	09/21/20		0.4793 (5)



https://aideadlin.es/?sub=NLP








## Installation

git clone https://github.com/mdda/worldtree_corpus.git
cd worldtree_corpus  # i.e. the REPO root directory
#git branch -a
git checkout -b textgraphs_2021 origin/textgraphs_2021

./run_setup.bash
# ./run_baseline.bash



# Do the train split on 2210 Questions : Takes ~420secs
./baseline_tfidf.py data-evalperiod/tables data-evalperiod/wt-expert-ratings.train.json > predict.baseline_tfidf.train.txt
# ./evaluate.py --gold data/wt-expert-ratings.dev.json predict.txt
./evaluate.py --gold data-evalperiod/wt-expert-ratings.train.json predict.baseline_tfidf.train.txt
# Mean NDCG Score : 0.5167861075889512



# Do the dev split on 496 Questions : Takes ~100secs
./baseline_tfidf.py data-evalperiod/tables data-evalperiod/wt-expert-ratings.dev.json > predict.baseline_tfidf.dev.txt
# ./evaluate.py --gold data/wt-expert-ratings.dev.json predict.txt
./evaluate.py --gold data-evalperiod/wt-expert-ratings.dev.json predict.baseline_tfidf.dev.txt
# Mean NDCG Score : 0.513046779054998  (agrees with 0.5130 NDCG given in the GitHub README.md)


# Do the text split on 1670 Questions # Actually :: Cannot, since the gold results are unknown to us
./baseline_tfidf.py data-evalperiod/tables data-evalperiod/wt-expert-ratings.test.json > predict.baseline_tfidf.test.txt
#./evaluate.py --gold data-evalperiod/wt-expert-ratings.test.json predict.baseline_tfidf.test.txt
# Guess at site evaluation on test set : 0.5010 (dustalov)
#   THEREFORE : The expected_test_score = train_score-0.0160
#   THEREFORE : The expected_test_score =   dev_score-0.0120



cd ../src
python baseline_retrieval.py  # Saves to ../predictions/predict.dev.baseline-retrieval.txt




python3 -m pip install --upgrade nni






# train.baseline.txt => 0.2469 MAP locally
# dev.baseline.txt   => 0.2550 MAP locally
# test.baseline.txt  => 0.2347 MAP (Test uploaded on 2020-08-31)

## ...

*  Need a lemmatizer to correctly resolve all 'node words' in the explanations
   *  Good resource : https://www.machinelearningplus.com/nlp/lemmatization-examples-python/
   
*  Can use this to identify nodes in Q&s 


git config --global credential.helper cache
git config --global credential.helper 'cache --timeout=36000'  # 10hrs




https://github.com/VHRanger/nodevectors
https://github.com/benedekrozemberczki/KarateClub
https://networkx.github.io/documentation/stable/reference/algorithms/shortest_paths.html


Applying Graph Neural Networks on Heterogeneous Nodes and Edge Features
  https://grlearning.github.io/papers/6.pdf


https://twitter.com/PetarV_93/status/1306689702020382720

https://petar-v.com/GAT/

  Gated Attention Networks (GaAN) (Zhang et al., 2018), 
    where gating mechanisms are inserted into the multi-head attention system of GATs, 
    in order to give different value to different headsâ€™ computations. 
    Several challenging baselines are outperformed on both 
    inductive node classification tasks (Reddit, PPI) and 
    a traffic speed forecasting task (METR-LA).


GRAPH CONVOLUTIONAL NETWORKS
  THOMAS KIPF, 30 SEPTEMBER 2016
    https://tkipf.github.io/graph-convolutional-networks/


CUDA=cu102
pip install torch-scatter==latest+${CUDA} -f https://pytorch-geometric.com/whl/torch-1.6.0.html
pip install torch-sparse==latest+${CUDA} -f https://pytorch-geometric.com/whl/torch-1.6.0.html
pip install torch-cluster==latest+${CUDA} -f https://pytorch-geometric.com/whl/torch-1.6.0.html
pip install torch-spline-conv==latest+${CUDA} -f https://pytorch-geometric.com/whl/torch-1.6.0.html
pip install torch-geometric

5095-dfd3-1847-a4a0 : TWICE in RELATIONSHIP.tsv
9b87-dd15-0cc5-32aa : TWICE in OPPOSITES.tsv
5689-a3ff-212f-560a : TWICE in OPPOSITES.tsv
b69d-9d08-0ad6-3023 : TWICE in UNIT.tsv

https://www.sbert.net/docs/usage/computing_sentence_embeddings.html





https://cloud.google.com/ai-platform/deep-learning-vm/docs/cloud-marketplace

asia-southeast1-a
asia-southeast1-b	Jurong West, Singapore, APAC	E2, N2, N2D, N1, M1, C2	Ivy Bridge, Sandy Bridge, Haswell, Broadwell, Skylake, Cascade Lake, AMD EPYC Rome	GPUs
asia-southeast1-c	Jurong West, Singapore, APAC	E2, N2, N2D, N1, M1, C2, A2	Ivy Bridge, Sandy Bridge, Haswell, Broadwell, Skylake, Cascade Lake, AMD EPYC Rome	GPUs

... only T4s...

us-west1-a
16vCPU 60Gb Mem
1 V100

Want access to V100 or A100 for research purposes : Have already been given Google credits under the GDE program to do so.  
Based in Singapore, but the instances there only seem to have T4s available : So, expanding search to find a close-ish alternative machine.

Rejected

I wanted a GPU quota increase from 0 to >0.  I am a Google Developer Expert for ML (GDE ML), which is a Google program, and co-organise the TensorFlow group in Singapore with more than 4000 members.  
I have credits generously granted to me by Google to conduct a research task.  It is mind-boggling to see that GCP is rejecting my request.







ssh andrewsm@simlim
cd /mnt/data/secure/mdda/rdai
git clone https://github.com/mdda/worldtree_corpus.git
cd worldtree_corpus  # i.e. the REPO root directory
#git branch -a
git checkout -b textgraphs_2021 origin/textgraphs_2021

./run_setup.bash
ln -s ../tg2021task/data-evalperiod/wt-expert-ratings.train.json data/wt-expert-ratings.train.json
ln -s ../tg2021task/data-evalperiod/wt-expert-ratings.dev.json data/wt-expert-ratings.dev.json
ln -s ../tg2021task/data-evalperiod/wt-expert-ratings.test.json data/wt-expert-ratings.test.json
ln -s ../tg2021task/data-evalperiod/tables data/tables

. ./env38/bin/activate
python baseline_bert.py
# 99% GPU utilisation, 4777Mb GPU RAM used : 8min30 per epoch, 5 epochs => 50mins for training
0.0675
0.0459
0.0341
0.0267
0.0215
# Dev output directly...  4mins


# On local machine:
#rsync -avz --progress predictions/predict.both.baseline-retrieval.hyperopt.tar.gz andrewsm@simlim:/mnt/data/secure/mdda/rdai/worldtree_corpus/predictions/

pushd predictions
tar -xzf predict.both.baseline-retrieval.hyperopt.tar.gz
popd

python model_mdda.py  # Use defaults - initial download : 440Mb
# 98% utilisation : 5559Mb of GPU RAM used
0.0677
0.0404
0.0372
0.0302
0.0275
0.0190






